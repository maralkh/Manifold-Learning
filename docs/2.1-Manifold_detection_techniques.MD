# üîç Complete Guide to Manifold Detection - Advanced Techniques

## üéØ Why is Manifold Detection So Important?

Imagine looking at your data from an angle where you can't see its true shape. Like looking at a sphere from one side - you might think it's flat!

### Concrete Example:
```
üåç Earth from space ‚Üí clearly spherical
üó∫Ô∏è World map on paper ‚Üí looks flat
üìç Your neighborhood ‚Üí appears linear
```

## üìä Method 1: Statistical Shape Analysis

### üî¥ Sphericity Test

**Core Idea**: If points lie on a sphere, they should all be at a constant distance from the center.

#### How it Works:

```python
def sphericity_test_explained(data):
    """Sphericity test with step-by-step explanations"""
    
    print("üîç Step 1: Finding the center")
    center = np.mean(data, axis=0)
    print(f"Estimated center: {center}")
    
    print("\nüìè Step 2: Computing distances")
    distances = np.linalg.norm(data - center, axis=1)
    mean_radius = np.mean(distances)
    std_radius = np.std(distances)
    
    print(f"Mean radius: {mean_radius:.3f}")
    print(f"Radius standard deviation: {std_radius:.3f}")
    print(f"Coefficient of variation: {std_radius/mean_radius:.3f}")
    
    print("\nüéØ Step 3: Analyzing spread")
    # If spherical, all distances should be approximately equal
    normalized_distances = distances / mean_radius
    
    # We expect normal distribution around 1
    deviation_from_unity = np.abs(normalized_distances - 1)
    mean_deviation = np.mean(deviation_from_unity)
    
    # Score: lower = more spherical
    sphericity_score = np.exp(-5 * mean_deviation)
    
    print(f"Mean deviation from 1: {mean_deviation:.3f}")
    print(f"Sphericity score: {sphericity_score:.3f}")
    
    # Visualize distance distribution
    import matplotlib.pyplot as plt
    plt.figure(figsize=(10, 4))
    
    plt.subplot(1, 2, 1)
    plt.hist(distances, bins=20, alpha=0.7, color='blue')
    plt.axvline(mean_radius, color='red', linestyle='--', 
                label=f'Mean = {mean_radius:.2f}')
    plt.xlabel('Distance from center')
    plt.ylabel('Frequency')
    plt.title('Distance Distribution')
    plt.legend()
    
    plt.subplot(1, 2, 2)
    plt.hist(normalized_distances, bins=20, alpha=0.7, color='green')
    plt.axvline(1, color='red', linestyle='--', label='Ideal value = 1')
    plt.xlabel('Normalized distance')
    plt.ylabel('Frequency')
    plt.title('Normalized Distances')
    plt.legend()
    
    plt.tight_layout()
    plt.show()
    
    return {
        'score': sphericity_score,
        'mean_radius': mean_radius,
        'radius_std': std_radius,
        'is_spherical': sphericity_score > 0.7
    }
```

#### Interpreting Results:
- **Score > 0.8**: Likely spherical
- **Score 0.5-0.8**: Possibly spherical with noise
- **Score < 0.5**: Likely non-spherical

### üç© Torus Detection

**Core Idea**: Torus = tube with two ends connected. Has two key characteristics:
1. **Major radius**: radius of the large ring
2. **Minor radius**: thickness of the tube

#### How to Detect It:

```python
def torus_detection_explained(data):
    """Torus detection with detailed explanations"""
    
    print("üç© Analyzing toroidal structure")
    
    center = np.mean(data, axis=0)
    centered_data = data - center
    
    if data.shape[1] >= 3:
        print("\nüîÑ Step 1: Radial distance analysis")
        # Distance from z-axis in xy plane
        radial_distances = np.sqrt(centered_data[:, 0]**2 + centered_data[:, 1]**2)
        
        print(f"Radial distance range: {np.min(radial_distances):.2f} - {np.max(radial_distances):.2f}")
        
        print("\nüìä Step 2: Distance distribution analysis")
        hist, bins = np.histogram(radial_distances, bins=30, density=True)
        
        # Find peaks
        peaks = []
        for i in range(1, len(hist)-1):
            if hist[i] > hist[i-1] and hist[i] > hist[i+1] and hist[i] > 0.1 * np.max(hist):
                peak_position = (bins[i] + bins[i+1]) / 2
                peaks.append(peak_position)
        
        print(f"Number of peaks found: {len(peaks)}")
        print(f"Peak positions: {peaks}")
        
        print("\nüåä Step 3: Vertical dimension analysis (z)")
        z_values = centered_data[:, 2]
        z_range = np.max(z_values) - np.min(z_values)
        z_std = np.std(z_values)
        
        print(f"Z value range: {z_range:.2f}")
        print(f"Z standard deviation: {z_std:.2f}")
        
        # Visualization
        fig = plt.figure(figsize=(15, 5))
        
        # Plot 1: Radial distance distribution
        plt.subplot(1, 3, 1)
        plt.hist(radial_distances, bins=30, alpha=0.7, color='orange')
        for peak in peaks:
            plt.axvline(peak, color='red', linestyle='--', alpha=0.8)
        plt.xlabel('Radial distance')
        plt.ylabel('Density')
        plt.title('Radial Distance Distribution')
        
        # Plot 2: Top view (xy plane)
        plt.subplot(1, 3, 2)
        plt.scatter(centered_data[:, 0], centered_data[:, 1], 
                   c=centered_data[:, 2], cmap='viridis', alpha=0.6)
        plt.xlabel('X')
        plt.ylabel('Y')
        plt.title('Top View (color = Z)')
        plt.colorbar()
        
        # Plot 3: Z distribution
        plt.subplot(1, 3, 3)
        plt.hist(z_values, bins=20, alpha=0.7, color='green')
        plt.xlabel('Z value')
        plt.ylabel('Frequency')
        plt.title('Z Value Distribution')
        
        plt.tight_layout()
        plt.show()
        
        # Scoring
        bimodality_score = min(len(peaks) / 2.0, 1.0)  # 2 peaks = ideal
        z_uniformity = np.exp(-z_std)  # expect uniform distribution
        
        torus_score = 0.7 * bimodality_score + 0.3 * z_uniformity
        
        print(f"\nüéØ Results:")
        print(f"Bimodality score: {bimodality_score:.3f}")
        print(f"Z uniformity score: {z_uniformity:.3f}")
        print(f"Final torus score: {torus_score:.3f}")
        
        return {
            'score': torus_score,
            'n_peaks': len(peaks),
            'peaks': peaks,
            'z_uniformity': z_uniformity
        }
```

### üìê Planarity Test

**Idea**: If data lies in a plane, the third dimension should be very small.

```python
def planarity_analysis_detailed(data):
    """Detailed planarity analysis"""
    
    print("üìê Analyzing data planarity")
    
    print("\nüîç Step 1: Principal Component Analysis (PCA)")
    pca = PCA()
    pca.fit(data)
    
    explained_var = pca.explained_variance_ratio_
    cumulative_var = np.cumsum(explained_var)
    
    print("Explained variance ratios:")
    for i, var in enumerate(explained_var):
        print(f"  Component {i+1}: {var:.4f} ({var*100:.1f}%)")
    
    print(f"\nCumulative variance of first 2 components: {cumulative_var[1]:.4f} ({cumulative_var[1]*100:.1f}%)")
    
    # Visualization
    fig = plt.figure(figsize=(12, 4))
    
    # Plot 1: Variance ratios
    plt.subplot(1, 3, 1)
    plt.bar(range(1, len(explained_var)+1), explained_var, alpha=0.7)
    plt.xlabel('Component number')
    plt.ylabel('Variance ratio')
    plt.title('Principal Component Variance')
    
    # Plot 2: Cumulative variance
    plt.subplot(1, 3, 2)
    plt.plot(range(1, len(cumulative_var)+1), cumulative_var, 'o-')
    plt.axhline(0.95, color='red', linestyle='--', label='95%')
    plt.xlabel('Number of components')
    plt.ylabel('Cumulative variance')
    plt.title('Cumulative Variance')
    plt.legend()
    
    # Plot 3: Projection onto main plane
    plt.subplot(1, 3, 3)
    data_2d = pca.transform(data)[:, :2]
    plt.scatter(data_2d[:, 0], data_2d[:, 1], alpha=0.6)
    plt.xlabel('Principal component 1')
    plt.ylabel('Principal component 2')
    plt.title('Projection onto Main Plane')
    
    plt.tight_layout()
    plt.show()
    
    # Calculate planarity score
    if len(explained_var) >= 3:
        planarity_score = 1 - explained_var[2]
        
        # Test distance to plane
        data_2d = pca.transform(data)[:, :2]
        data_reconstructed = pca.inverse_transform(
            np.column_stack([data_2d, np.zeros(len(data_2d))])
        )
        
        distances_to_plane = np.linalg.norm(data - data_reconstructed, axis=1)
        mean_distance = np.mean(distances_to_plane)
        
        print(f"\nüìè Average distance to optimal plane: {mean_distance:.4f}")
        
        # Refine score based on distance
        distance_factor = np.exp(-10 * mean_distance)
        final_score = planarity_score * distance_factor
        
    else:
        final_score = 0.9 if len(explained_var) <= 2 else 0.0
    
    print(f"\nüéØ Planarity score: {final_score:.3f}")
    
    return {
        'score': final_score,
        'explained_variance': explained_var,
        'principal_components': pca.components_
    }
```

## üåê Method 2: Intrinsic Dimension Estimation

### üé≤ Correlation Dimension

**Idea**: Count how many points we have up to distance Œµ, then see how this number grows with Œµ.

```python
def correlation_dimension_explained(data, max_eps=None):
    """Correlation dimension calculation with complete explanations"""
    
    print("üé≤ Computing Correlation Dimension")
    
    # Compute distance matrix
    print("üìä Computing distance matrix...")
    pairwise_dists = pdist(data)
    
    if max_eps is None:
        max_eps = np.percentile(pairwise_dists, 50)  # half of median
    
    eps_range = np.logspace(
        np.log10(np.min(pairwise_dists[pairwise_dists > 0])),
        np.log10(max_eps),
        30
    )
    
    correlations = []
    
    for eps in eps_range:
        count = np.sum(pairwise_dists < eps)
        total_pairs = len(pairwise_dists)
        correlation = count / total_pairs
        correlations.append(correlation)
    
    # Fit line in log-log plot
    log_eps = np.log10(eps_range)
    log_corr = np.log10(np.array(correlations) + 1e-10)
    
    # Filter valid points
    valid_mask = np.isfinite(log_corr) & (np.array(correlations) > 1e-6)
    
    if np.sum(valid_mask) > 5:
        slope, intercept = np.polyfit(log_eps[valid_mask], log_corr[valid_mask], 1)
        correlation_dim = slope
        
        # Visualization
        plt.figure(figsize=(10, 4))
        
        plt.subplot(1, 2, 1)
        plt.loglog(eps_range, correlations, 'bo-', alpha=0.7)
        plt.xlabel('Œµ (distance)')
        plt.ylabel('C(Œµ) (correlation)')
        plt.title('Correlation Function')
        plt.grid(True, alpha=0.3)
        
        plt.subplot(1, 2, 2)
        plt.plot(log_eps, log_corr, 'ro', alpha=0.7, label='Data')
        fit_line = slope * log_eps + intercept
        plt.plot(log_eps, fit_line, 'b-', label=f'Slope = {slope:.2f}')
        plt.xlabel('log‚ÇÅ‚ÇÄ(Œµ)')
        plt.ylabel('log‚ÇÅ‚ÇÄ(C(Œµ))')
        plt.title('Log-Log Plot')
        plt.legend()
        plt.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.show()
        
        print(f"üéØ Estimated correlation dimension: {correlation_dim:.2f}")
        
    else:
        correlation_dim = data.shape[1]
        print("‚ö†Ô∏è Insufficient data for reliable estimation")
    
    return max(1, min(correlation_dim, data.shape[1]))
```

### üßÆ Maximum Likelihood Estimation (MLE)

**Idea**: Use the ratio of farthest neighbor distance to nearest neighbor distance to estimate dimension.

```python
def mle_dimension_explained(data, k=12):
    """MLE dimension estimation with explanations"""
    
    print(f"üßÆ MLE Dimension Estimation (k={k})")
    
    from sklearn.neighbors import NearestNeighbors
    
    nbrs = NearestNeighbors(n_neighbors=k+1).fit(data)
    distances, indices = nbrs.kneighbors(data)
    
    # Remove self-distance (which is zero)
    distances = distances[:, 1:]
    
    print(f"üìè Found {k} nearest neighbors for each point")
    
    log_ratios = []
    
    for i in range(len(data)):
        r_k = distances[i, -1]  # Farthest neighbor
        r_1 = distances[i, 0]   # Nearest neighbor
        
        if r_1 > 1e-10:  # Prevent log(0)
            log_ratio = np.log(r_k / r_1)
            log_ratios.append(log_ratio)
    
    if log_ratios:
        mean_log_ratio = np.mean(log_ratios)
        mle_dim = (k-1) / mean_log_ratio if mean_log_ratio > 0 else data.shape[1]
        
        print(f"üìä Mean log(r_k/r_1): {mean_log_ratio:.3f}")
        print(f"üéØ Estimated MLE dimension: {mle_dim:.2f}")
        
        # Visualize log ratio distribution
        plt.figure(figsize=(8, 4))
        plt.hist(log_ratios, bins=30, alpha=0.7, color='purple')
        plt.axvline(mean_log_ratio, color='red', linestyle='--', 
                   label=f'Mean = {mean_log_ratio:.3f}')
        plt.xlabel('log(r_k / r_1)')
        plt.ylabel('Frequency')
        plt.title('Log Distance Ratio Distribution')
        plt.legend()
        plt.show()
        
    else:
        mle_dim = data.shape[1]
        print("‚ö†Ô∏è Cannot compute MLE")
    
    return max(1, min(mle_dim, data.shape[1]))
```

## üîÑ Method 3: Curvature Analysis

**Idea**: Different manifolds have different curvatures:
- Sphere: constant positive curvature
- Plane: zero curvature  
- Torus: variable curvature

```python
def curvature_analysis_detailed(data, k=15):
    """Detailed local curvature analysis"""
    
    print(f"üîÑ Local curvature analysis (k={k})")
    
    from sklearn.neighbors import NearestNeighbors
    
    nbrs = NearestNeighbors(n_neighbors=k).fit(data)
    distances, indices = nbrs.kneighbors(data)
    
    local_curvatures = []
    principal_curvatures = []
    
    for i in range(0, len(data), max(1, len(data)//50)):  # Sample for speed
        
        # Local neighbors
        neighbor_indices = indices[i, 1:]  # Remove self
        local_points = data[neighbor_indices]
        center_point = data[i]
        
        # Center the points
        centered_points = local_points - center_point
        
        if len(centered_points) >= 3:
            # Local PCA
            pca = PCA()
            pca.fit(centered_points)
            
            # Curvature from eigenvalue ratios
            eigenvals = pca.explained_variance_
            
            if len(eigenvals) >= 2:
                # Principal curvature = ratio of smallest to largest eigenvalue
                if eigenvals[0] > 1e-10:
                    curvature = eigenvals[-1] / eigenvals[0]
                    local_curvatures.append(curvature)
                    
                    # Principal curvatures (if 3D)
                    if len(eigenvals) >= 3:
                        k1 = eigenvals[1] / eigenvals[0] if eigenvals[0] > 1e-10 else 0
                        k2 = eigenvals[2] / eigenvals[0] if eigenvals[0] > 1e-10 else 0
                        principal_curvatures.append((k1, k2))
    
    if local_curvatures:
        mean_curvature = np.mean(local_curvatures)
        curvature_std = np.std(local_curvatures)
        max_curvature = np.max(local_curvatures)
        
        print(f"üìä Curvature statistics:")
        print(f"  Mean: {mean_curvature:.4f}")
        print(f"  Standard deviation: {curvature_std:.4f}")
        print(f"  Maximum: {max_curvature:.4f}")
        print(f"  Coefficient of variation: {curvature_std/mean_curvature:.3f}")
        
        # Interpretation
        if curvature_std < 0.1 * mean_curvature:
            shape_type = "Likely sphere (uniform curvature)"
        elif mean_curvature < 0.01:
            shape_type = "Likely flat (low curvature)"
        elif curvature_std > 0.5 * mean_curvature:
            shape_type = "Likely torus or complex shape (variable curvature)"
        else:
            shape_type = "Uncertain"
        
        print(f"üéØ Shape detection: {shape_type}")
        
        # Visualization
        plt.figure(figsize=(12, 4))
        
        plt.subplot(1, 3, 1)
        plt.hist(local_curvatures, bins=30, alpha=0.7, color='teal')
        plt.axvline(mean_curvature, color='red', linestyle='--',
                   label=f'Mean = {mean_curvature:.3f}')
        plt.xlabel('Local curvature')
        plt.ylabel('Frequency')
        plt.title('Local Curvature Distribution')
        plt.legend()
        
        # If 3D, show principal curvatures
        if principal_curvatures:
            k1_vals = [k[0] for k in principal_curvatures]
            k2_vals = [k[1] for k in principal_curvatures]
            
            plt.subplot(1, 3, 2)
            plt.scatter(k1_vals, k2_vals, alpha=0.6)
            plt.xlabel('Principal curvature 1')
            plt.ylabel('Principal curvature 2')
            plt.title('Principal Curvatures')
            plt.axhline(0, color='gray', linestyle='-', alpha=0.3)
            plt.axvline(0, color='gray', linestyle='-', alpha=0.3)
        
        plt.subplot(1, 3, 3)
        sorted_curvatures = np.sort(local_curvatures)
        plt.plot(sorted_curvatures, alpha=0.8)
        plt.xlabel('Rank')
        plt.ylabel('Curvature')
        plt.title('Sorted Curvatures')
        
        plt.tight_layout()
        plt.show()
        
        return {
            'mean_curvature': mean_curvature,
            'curvature_std': curvature_std,
            'max_curvature': max_curvature,
            'shape_interpretation': shape_type
        }
```

## üéµ Method 4: Spectral and Frequency Analysis

### For detecting periodic patterns (like hypertori):

```python
def spectral_analysis_complete(data):
    """Complete spectral analysis for pattern detection"""
    
    print("üéµ Spectral analysis of data")
    
    # Frequency analysis for each feature
    periodicity_results = []
    
    for feature_idx in range(data.shape[1]):
        feature = data[:, feature_idx]
        feature_name = f"Feature {feature_idx + 1}"
        
        print(f"\nüîç {feature_name}:")
        
        # Zero-mean
        feature_centered = feature - np.mean(feature)
        
        # FFT
        fft_vals = np.fft.fft(feature_centered)
        freqs = np.fft.fftfreq(len(feature), d=1.0)
        
        # Power spectrum (positive frequencies only)
        power_spectrum = np.abs(fft_vals[:len(fft_vals)//2])
        positive_freqs = freqs[:len(freqs)//2]
        
        if len(power_spectrum) > 1:
            # Remove DC component
            power_spectrum = power_spectrum[1:]
            positive_freqs = positive_freqs[1:]
            
            if len(power_spectrum) > 0:
                # Find dominant peak
                max_power_idx = np.argmax(power_spectrum)
                dominant_freq = positive_freqs[max_power_idx]
                max_power = power_spectrum[max_power_idx]
                mean_power = np.mean(power_spectrum)
                
                # Signal-to-noise ratio
                snr = max_power / (mean_power + 1e-10)
                
                print(f"  Dominant frequency: {abs(dominant_freq):.4f}")
                print(f"  Signal-to-noise ratio: {snr:.2f}")
                
                periodicity_results.append({
                    'feature': feature_name,
                    'dominant_freq': abs(dominant_freq),
                    'snr': snr,
                    'is_periodic': snr > 3.0
                })
                
                # Visualization
                plt.figure(figsize=(12, 4))
                
                plt.subplot(1, 3, 1)
                plt.plot(feature, alpha=0.8)
                plt.title(f'{feature_name} - Original Signal')
                plt.xlabel('Sample')
                plt.ylabel('Value')
                
                plt.subplot(1, 3, 2)
                plt.plot(positive_freqs, power_spectrum, alpha=0.8)
                plt.axvline(abs(dominant_freq), color='red', linestyle='--',
                           label=f'Dominant freq = {abs(dominant_freq):.3f}')
                plt.xlabel('Frequency')
                plt.ylabel('Power spectrum')
                plt.title('Power Spectrum')
                plt.legend()
                plt.yscale('log')
                
                plt.subplot(1, 3, 3)
                # Phase spectrum
                phase_spectrum = np.angle(fft_vals[:len(fft_vals)//2])
                plt.plot(positive_freqs, phase_spectrum[1:], alpha=0.8)
                plt.xlabel('Frequency')
                plt.ylabel('Phase')
                plt.title('Phase Spectrum')
                
                plt.tight_layout()
                plt.show()
    
    # Summary of results
    if periodicity_results:
        periodic_features = [r for r in periodicity_results if r['is_periodic']]
        mean_snr = np.mean([r['snr'] for r in periodicity_results])
        
        print(f"\nüéØ Spectral analysis summary:")
        print(f"  Features with periodic patterns: {len(periodic_features)}/{len(periodicity_results)}")
        print(f"  Average SNR: {mean_snr:.2f}")
        
        if len(periodic_features) >= 2 and mean_snr > 5:
            manifold_suggestion = "Likely hypertorus (multi-dimensional periodic)"
        elif len(periodic_features) == 1:
            manifold_suggestion = "Likely torus or shape with one periodic dimension"
        else:
            manifold_suggestion = "No clear periodic pattern observed"
            
        print(f"  Suggestion: {manifold_suggestion}")
        
        return {
            'periodicity_results': periodicity_results,
            'mean_snr': mean_snr,
            'n_periodic_features': len(periodic_features),
            'manifold_suggestion': manifold_suggestion
        }

def graph_laplacian_analysis(data, n_neighbors=10):
    """Graph Laplacian analysis for understanding topology"""
    
    print(f"üï∏Ô∏è Graph Laplacian analysis (k={n_neighbors})")
    
    from sklearn.neighbors import kneighbors_graph
    
    # Build k-NN graph
    connectivity = kneighbors_graph(
        data, n_neighbors=n_neighbors, 
        mode='distance', include_self=False
    )
    
    # Convert to symmetric matrix
    adjacency = 0.5 * (connectivity + connectivity.T)
    adjacency_dense = adjacency.toarray()
    
    # Compute Laplacian
    degrees = np.array(adjacency_dense.sum(axis=1)).flatten()
    degree_matrix = np.diag(degrees)
    laplacian = degree_matrix - adjacency_dense
    
    # Normalization
    degree_sqrt_inv = np.diag(1.0 / np.sqrt(degrees + 1e-10))
    normalized_laplacian = degree_sqrt_inv @ laplacian @ degree_sqrt_inv
    
    # Eigenvalues
    eigenvals, eigenvecs = np.linalg.eigh(normalized_laplacian)
    
    # Sort
    idx = np.argsort(eigenvals)
    eigenvals = eigenvals[idx]
    eigenvecs = eigenvecs[:, idx]
    
    print(f"üìä First few eigenvalues:")
    for i in range(min(10, len(eigenvals))):
        print(f"  Œª_{i+1} = {eigenvals[i]:.6f}")
    
    # Analyze spectral gap
    eigenvalue_gaps = np.diff(eigenvals)
    max_gap_idx = np.argmax(eigenvalue_gaps)
    max_gap = eigenvalue_gaps[max_gap_idx]
    
    print(f"\nüéØ Largest spectral gap:")
    print(f"  Between Œª_{max_gap_idx+1} and Œª_{max_gap_idx+2}")
    print(f"  Gap size: {max_gap:.6f}")
    print(f"  Estimated intrinsic dimension: {max_gap_idx + 1}")
    
    # Visualization
    fig = plt.figure(figsize=(15, 5))
    
    # Plot 1: Eigenvalues
    plt.subplot(1, 3, 1)
    plt.plot(eigenvals[:min(50, len(eigenvals))], 'o-', alpha=0.8)
    plt.axvline(max_gap_idx + 1, color='red', linestyle='--', 
               label=f'Largest gap at {max_gap_idx+1}')
    plt.xlabel('Eigenvalue number')
    plt.ylabel('Eigenvalue')
    plt.title('Laplacian Spectrum')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    # Plot 2: Gaps
    plt.subplot(1, 3, 2)
    plt.plot(eigenvalue_gaps[:min(20, len(eigenvalue_gaps))], 'o-', alpha=0.8)
    plt.axvline(max_gap_idx, color='red', linestyle='--',
               label=f'Maximum = {max_gap:.4f}')
    plt.xlabel('Gap number')
    plt.ylabel('Gap size')
    plt.title('Spectral Gaps')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    # Plot 3: Fiedler vector (second eigenvector)
    plt.subplot(1, 3, 3)
    fiedler_vector = eigenvecs[:, 1]  # Second eigenvector
    plt.plot(fiedler_vector, alpha=0.8)
    plt.xlabel('Point number')
    plt.ylabel('Value')
    plt.title('Fiedler Vector (2nd eigenvector)')
    plt.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.show()
    
    # Interpret results
    if eigenvals[1] < 1e-6:
        connectivity_status = "Disconnected graph (multiple connected components)"
    elif max_gap > 0.1:
        connectivity_status = f"Likely {max_gap_idx + 1} main clusters"
    else:
        connectivity_status = "Well connected"
    
    print(f"üîó Connectivity status: {connectivity_status}")
    
    return {
        'eigenvalues': eigenvals,
        'eigenvectors': eigenvecs,
        'spectral_gap': max_gap,
        'gap_position': max_gap_idx,
        'intrinsic_dimension': max_gap_idx + 1,
        'connectivity_status': connectivity_status
    }
```

## ü§ñ Unified Detection System

```python
class AdvancedManifoldDetector:
    """Advanced manifold detector with comprehensive analysis"""
    
    def __init__(self):
        self.test_results = {}
        self.confidence_weights = {
            'sphere': {'sphericity': 0.4, 'curvature_uniformity': 0.3, 'spectral': 0.3},
            'torus': {'toroidal': 0.5, 'curvature_variation': 0.3, 'periodicity': 0.2},
            'plane': {'planarity': 0.6, 'low_curvature': 0.4},
            'hypertorus': {'periodicity': 0.7, 'high_dimension': 0.3},
            'complex': {'spectral_complexity': 0.5, 'nonlinearity': 0.5}
        }
    
    def full_analysis(self, data, visualize=True):
        """Complete analysis with all methods"""
        
        print("üî¨ Starting comprehensive manifold analysis...")
        print("=" * 50)
        
        # Preprocessing
        data_clean = self._robust_preprocessing(data)
        
        # Run all tests
        self.test_results = {}
        
        print("\n1Ô∏è‚É£ Sphericity test...")
        self.test_results['sphericity'] = sphericity_test_explained(data_clean)
        
        print("\n2Ô∏è‚É£ Torus test...")
        self.test_results['toroidal'] = torus_detection_explained(data_clean)
        
        print("\n3Ô∏è‚É£ Planarity test...")
        self.test_results['planarity'] = planarity_analysis_detailed(data_clean)
        
        print("\n4Ô∏è‚É£ Intrinsic dimension estimation...")
        self.test_results['intrinsic_dim'] = {
            'correlation': correlation_dimension_explained(data_clean),
            'mle': mle_dimension_explained(data_clean),
            'pca': pca_dimension(data_clean)
        }
        
        print("\n5Ô∏è‚É£ Curvature analysis...")
        self.test_results['curvature'] = curvature_analysis_detailed(data_clean)
        
        print("\n6Ô∏è‚É£ Spectral analysis...")
        self.test_results['spectral_freq'] = spectral_analysis_complete(data_clean)
        self.test_results['spectral_graph'] = graph_laplacian_analysis(data_clean)
        
        # Compute final scores
        final_scores = self._compute_comprehensive_scores()
        
        # Select best
        best_manifold = max(final_scores.keys(), key=lambda k: final_scores[k])
        confidence = final_scores[best_manifold]
        
        print("\n" + "=" * 50)
        print("üéØ Final Results:")
        print(f"Detected manifold: {best_manifold}")
        print(f"Confidence: {confidence:.3f}")
        print(f"Status: {'‚úÖ Confident' if confidence > 0.7 else '‚ùì Uncertain'}")
        
        print("\nüìä All option scores:")
        for manifold, score in sorted(final_scores.items(), key=lambda x: x[1], reverse=True):
            print(f"  {manifold}: {score:.3f}")
        
        return {
            'detected_manifold': best_manifold,
            'confidence': confidence,
            'all_scores': final_scores,
            'detailed_results': self.test_results
        }
    
    def _robust_preprocessing(self, data):
        """Robust preprocessing"""
        
        # Remove NaNs
        data_clean = data[~np.isnan(data).any(axis=1)]
        
        # Remove outliers with IQR method
        Q1 = np.percentile(data_clean, 25, axis=0)
        Q3 = np.percentile(data_clean, 75, axis=0)
        IQR = Q3 - Q1
        
        lower_bound = Q1 - 2.0 * IQR  # Slightly more loose
        upper_bound = Q3 + 2.0 * IQR
        
        mask = np.all((data_clean >= lower_bound) & (data_clean <= upper_bound), axis=1)
        data_clean = data_clean[mask]
        
        # Normalize if necessary
        scales = np.std(data_clean, axis=0)
        if np.max(scales) / np.min(scales) > 10:  # If scales are very different
            from sklearn.preprocessing import StandardScaler
            scaler = StandardScaler()
            data_clean = scaler.fit_transform(data_clean)
        
        print(f"üìã Preprocessed data: {len(data_clean)} points from {len(data)} original points")
        
        return data_clean
    
    def _compute_comprehensive_scores(self):
        """Compute comprehensive scores"""
        
        scores = {}
        
        # Sphere
        sphericity = self.test_results['sphericity']['score']
        curvature_uniformity = 1 / (1 + self.test_results['curvature']['curvature_std'])
        intrinsic_match = 1.0 if np.mean(list(self.test_results['intrinsic_dim'].values())) <= 3 else 0.5
        
        scores['Sphere'] = 0.5 * sphericity + 0.3 * curvature_uniformity + 0.2 * intrinsic_match
        
        # Torus
        toroidal = self.test_results['toroidal']['score']
        curvature_var = min(self.test_results['curvature']['curvature_std'], 1.0)
        
        scores['Torus'] = 0.7 * toroidal + 0.3 * curvature_var
        
        # Plane/Disk
        planarity = self.test_results['planarity']['score']
        low_curvature = np.exp(-self.test_results['curvature']['mean_curvature'])
        
        scores['Plane/Disk'] = 0.7 * planarity + 0.3 * low_curvature
        
        # Hypertorus
        if 'periodicity_results' in self.test_results['spectral_freq']:
            n_periodic = self.test_results['spectral_freq']['n_periodic_features']
            mean_snr = self.test_results['spectral_freq']['mean_snr']
            periodicity_score = min(n_periodic / data.shape[1], 1.0) * min(mean_snr / 10, 1.0)
        else:
            periodicity_score = 0
            
        high_dim = 1.0 if np.mean(list(self.test_results['intrinsic_dim'].values())) > 3 else 0.3
        
        scores['Hypertorus'] = 0.8 * periodicity_score + 0.2 * high_dim
        
        # Complex manifold
        spectral_gap = min(self.test_results['spectral_graph']['spectral_gap'], 1.0)
        nonlinearity = 1 - scores['Plane/Disk']
        
        scores['Complex Manifold'] = 0.6 * spectral_gap + 0.4 * nonlinearity
        
        return scores

# Helper functions
def pca_dimension(data, variance_threshold=0.95):
    """PCA dimension with variance threshold"""
    
    pca = PCA()
    pca.fit(data)
    
    cumulative_variance = np.cumsum(pca.explained_variance_ratio_)
    intrinsic_dim = np.argmax(cumulative_variance >= variance_threshold) + 1
    
    return intrinsic_dim

# Comprehensive test
def comprehensive_test():
    """Comprehensive system test"""
    
    print("üß™ Comprehensive Manifold Detection System Test")
    print("=" * 60)
    
    # Generate test data
    np.random.seed(123)
    
    # Sphere
    n = 300
    theta = np.random.uniform(0, np.pi, n)
    phi = np.random.uniform(0, 2*np.pi, n)
    sphere_data = np.column_stack([
        2 * np.sin(theta) * np.cos(phi),
        2 * np.sin(theta) * np.sin(phi),
        2 * np.cos(theta)
    ]) + np.random.normal(0, 0.1, (n, 3))
    
    detector = AdvancedManifoldDetector()
    result = detector.full_analysis(sphere_data)

if __name__ == "__main__":
    comprehensive_test()
```

## üéì Key Points for Usage:

### ‚úÖ When to Use Which Method:

1. **Low-dimensional data (2D-3D)**: Start with Statistical Shape Analysis
2. **High-dimensional data**: Focus on intrinsic dimension estimation  
3. **Noisy data**: Use robust methods
4. **Periodic patterns**: Frequency analysis

### ‚ö†Ô∏è Limitations:

- **Data volume**: Spectral methods are slow for large datasets
- **Noise**: Requires appropriate preprocessing  
- **Parameter tuning**: k in k-NN, number of bins, etc.
- **Interpretation**: Scores are relative, not absolute

### üí° Practical Tips:

1. **Always combine multiple methods**
2. **Visually inspect results** 
3. **Use domain knowledge**
4. **Test with synthetic data**

## üîß Usage Examples:

### Example 1: Sphere Detection
```python
# Generate sphere data
theta = np.random.uniform(0, np.pi, 500)
phi = np.random.uniform(0, 2*np.pi, 500)
sphere_data = np.column_stack([
    np.sin(theta) * np.cos(phi),
    np.sin(theta) * np.sin(phi),
    np.cos(theta)
]) + np.random.normal(0, 0.05, (500, 3))

# Detect
detector = AdvancedManifoldDetector()
result = detector.full_analysis(sphere_data)
print(f"Detected: {result['detected_manifold']}")
```

### Example 2: Torus Detection
```python
# Generate torus data
u = np.random.uniform(0, 2*np.pi, 400)
v = np.random.uniform(0, 2*np.pi, 400)
R, r = 2, 0.5
torus_data = np.column_stack([
    (R + r * np.cos(v)) * np.cos(u),
    (R + r * np.cos(v)) * np.sin(u),
    r * np.sin(v)
]) + np.random.normal(0, 0.08, (400, 3))

result = detector.full_analysis(torus_data)
print(f"Detected: {result['detected_manifold']}")
```

### Example 3: Swiss Roll (Complex Manifold)
```python
# Generate Swiss roll
t = 3 * np.pi/2 * (1 + 2 * np.random.uniform(0, 1, 300))
height = 30 * np.random.uniform(0, 1, 300)
swiss_roll = np.column_stack([
    t * np.cos(t),
    height,
    t * np.sin(t)
]) + np.random.normal(0, 0.1, (300, 3))

result = detector.full_analysis(swiss_roll)
print(f"Detected: {result['detected_manifold']}")
```

This comprehensive guide provides you with all the tools needed to detect and analyze different types of manifolds in your data!